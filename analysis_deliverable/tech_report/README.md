# Tech Report
This is where you can type out your tech report.

### A defined hypothesis or prediction task, with clearly stated metrics for success.

1. 
H_0: There is no significant correlation between a song’s Danceability score and whether they are on the billboard for more than 10 weeks.

H_a: There is a significant correlation between a song’s Danceability score and whether they are on the billboard for more than 10 weeks.

2. 
H_0: The average Valence score for songs with 10 weeks or more on the Billboard is not significantly different than the overall average Valence score for songs that have ever been on the Billboard.

H_a: The average Valence score for songs with 10 weeks or more on the Billboard is significantly different than the overall average Valence score for songs that have ever been on the Billboard. 

3. 
H_0: The average Energy score for songs in the 1970s is not significantly different from the average Energy score for songs in the 2010s.

H_a: The average Energy score for songs in the 1970s is significantly different from the average Energy score for songs in the 2010s. 


### Why did you use this statistical test or ML algorithm? 

For the first hypothesis, we’re using the chi-square independence test to check if danceability is related to a song being on the Billboard chart for an extended period of time (period of 10 weeks). Intuitively, there should be a relationship between these two variables and songs with higher danceability value should stay on the chart longer. Danceable songs are more desirable for  social events and they tend to be played more often, resulting in longer weeks staying on the chart. Using a chi-square independence test is best here because we want to test if there is a relationship between the two variables. 

For the second hypothesis, we’re using a one-sample t-test to check if the valence (positivity) of songs that are on the Billboard for a long time are statistically different from the songs that are only on the Billboard for a relatively short amount of time. The intuition behind this question is that songs with more positivity may have a different impact on people’s willingness to keep playing the song. If a song is more positive in mood, it may be true that it is more palatable to listen to over time, hence raising its resilience to stay on the Billboard for longer.

For the third hypothesis, we’re using a two-sample t-test to check if the energy in the songs from the 1980s decade is significantly different from the energy in the songs from the 2010s. The intuition behind this question is that as music production has evolved to use more electronic components, there seems to be a dramatic increase in the overall energy a song can produce. Using a two-sample t-test is best here because we want to compare whether the means of the Energy score from two different samples are different from each other.

For the ML part, we decided to use a feedforward neural network with 4 layers. Our initial thought was to use linear regression, but it did not fit our dataset well. Our goal is to predict how many weeks a song can stay on Billboard Hot 100 given its attributes, and our dataset contains information about songs that have made it to Billboard Hot 100 since 1970s, their attributes and the number of weeks they have been on the chart. It makes sense for us to use FFNN to train our model using our dataset and then use it to predict.


### Which other tests did you consider or evaluate? 

For the hypothesis testing part, we mainly looked at the tests that were covered in the stats homework and it addressed the need for our hypothesis. For ML part, we also considered linear regression, but as mentioned above and also in the upcoming questions, our dataset did not fit it well and the output wasn’t desirable. 


### How did you measure success or failure? Why that metric/value? What challenges did you face evaluating the model? Did you have to clean or restructure your data?

For the hypothesis testing tests we’re using, we would first consider their statistical significance so that we can interpret the results from the chi-square / t-tests. We choose to center the success of our tests around statistical significance as the results of these tests compounded with our visualizations reinforced the determined status of hypothesis and together seemed to enhance the accuracy of our results. For hypotheses involving correlation between measured attributes, these tests were compared to a heatmap generated by the matplotlib library using our complete data set to ensure the raw validity of our results. For hypothesis involving comparisons between recorded decades, these tests were compared to our bar graph visualizations as they showed clear trends in unbiasedly either supporting or denouncing our hypthesized claims. Throughout the evaluation of our model we begun to realize that bugs in the dependencies we were using were causing certain attributes in the 2000’s and 2010’s decades to not be included in our results. These occurrences caused wide shifts in our results as certains tests would face huge shifts in their statistical significance and cause us to reconsider the interpretation of our results.


ML Lasso- We define success by seeing how well lasso regression eliminates irrelevant attributes. We compare the weights produced from lasso regression to results from the hypothesis testing part (heat map and chi-squared tests for instance) and see how well they match. 

ML FFNN- We define success by seeing how close our predictions are to the actual number of weeks a song stays on Billboard. This can be validated by the loss function, which should converge to zero if our model is successful. However, unlike the lasso regression where there exists a hard threshold for determining success (weight = 0 means the song attribute is irrelevant), we do not have as clear a threshold for FFNN. The final loss value we get is a bit high for us to consider our model as good enough (about 40)

We cleaned our data by eliminating songs that we cannot get attributes from Spotify. If there’s not an exact match (for instance, the song on Billboard has a feat singer but it does not on Spotify), we use the alternative version with Spotify (the version without the feat singer). When the song just does not exist on Spotify, we take that song out of the dataset. We did normalize the data so that the accuracy of our model could improve.


### What is your interpretation of the results? Do you accept or deny the hypothesis, or are you satisfied with your prediction accuracy? For prediction projects, we expect you to argue why you got the accuracy/success metric you have. Intuitively, how do you react to the results? Are you confident in the results?

Hypothesis testing-

1. chi-square test. Our test statistic is 113334 with a p-value of 1.68*10^-28. This means that our test is statistically significant and we can reject our null hypothesis. It suggests that there is a significant correlation between the song’s danceability and the number of weeks it remains on Billboard Hot 100.

2. Two-sample t-test. We looked at the average valence score for songs that were on the chart for longer times (top 10 percentile for weeks_on_chart) and shorter times (bottom 90 percentile for weeks_on_chart) for every decade (1970s to 2020s). For 1970s, our test statistic is -0.21 with a p-value of 0.83. For 1980s, our test statistic is -0.51 with a p-value of 0.61. For 1990s, our test statistic is -0.30 with a p-value of 0.76. For 2000s, our test statistic is 1.04 with a p-value of 0.29. For 2010s, our test statistic is 2.65 with a p-value of 0.008. For 2020s, our test statistic is 3.52 with a p-value of 0.0004. For overall (1970s - 2020s), our test statistic is -3.39 with a p-value of 0.0006.
Tests for 2010s, 2020s and overall had a statistically significant p-value, which means that we will reject the null hypothesis. It suggests that in those three time periods, there is a difference for the song valence score for songs that stayed longer on the Billboard.

3. Two-sample t-test. We looked at the average energy score for songs in the 1970s and 2010s. Our test statistic is -20.15 with a p-value of 2.91*10^-88. This means that our test is statistically significant and we can reject our null hypothesis. It suggests that there is no significant diffference between the song’s danceability and the number of weeks it remains on Billboard Hot 100. This is also supported by our energy bar chart plot.

ML Lasso - The results did not fit the dataset as much as we expected. We think that it’s because the features we used as inputs do not have a strong enough correlation. Initially, we thought that even though outside features such as artists’ fame and social trend would impact the number of weeks a song remains on Billboard, it would not have that big of an impact. However, it seems like from the results, song features by themselves are not sufficient to make accurate predictions. 

ML FFNN- Unfortunately, FFNN results also did not fit the dataset as much as we expected. Our model architecture might not be complicated enough to produce accurate predictions. Instead of thoroughly learning through input data and making predictions, it’s just predicting average values, which indeed minimizes the loss function.

We first implemented lasso regression and noticed that a lot of attributes converged to zero. Noticing that this method did not work as well as we expected, we did some research and switched to FFNN, which should be able to fit into any dataset. However, the results are still unsatisfactory despite that we tried to add more layers and more trainable parameters.



### For your visualization, why did you pick this graph? What alternative ways might you communicate the result? Were there any challenges visualizing the results, if so, what where they? Will your visualization require text to provide context or is it standalone (either is fine, but it's recognize which type your visualization is)?

Our project results have been visualized through a heat map map demonstrating correlation between the measured characteristics of songs and a bar chart representing the averages of these characteristics over our observed time period. An alternative to visualizing the results of our test and data could be displayed through line graphs which would also show correlated trends in observed attributes and change over time. 

We considered doing a histogram as an alternative to the bar chart, however, the issues with histograms is that they only show observed frequencies while we instead aimed at showing trends in averages of each attribute over each individual decade.  

We will need to provide general context for audiences to understand our graph. We need to specify the goal of our project and explain the axis. Specifically, we need to communicate to our audience that we are looking at songs that made to the Billboard Hot 100 since the 1970s and we are using song attributes from Spotify. The attributes are numerical values that analyzes the characteristics of the song. From there, audiences should be able to understand our graph.





### Full results + graphs (at least 1 stats/ml test and at least 1 visualization). You should push your visualizations to the /analysis_deliverable/visualizations folder in your repo. Depending on your model/test/project we would ideally like you to show us your full process so we can evaluate how you conducted the test!

See visualization folder under analysis_deliverable in the repository


### If you did a statistics test, are there any confounding trends or variables you might be observing?
 
Testing our second hypothesis yielded varying results as we shifted the decades of data included in this analysis. When testing this hypothesis on data from the 2010’s and 2020’s against the entire data set (70s-2020’s) we yielded a p-value indicating statistical significance causing us to reject our null hypothesis. However, when testing our second hypothesis on data from the 70’s to the 2000’s, we yielded a p-value that was statistically insignificant and contradictory to our testing of just 2010’s and 2020’s data. This finding was interesting as our perspectives on our second hypothesis changed depending on the perspective on the overall data.

During our statistical testing and analysis, we were surprised to discover very limited correlations between the measured valence and tempo of an observed song. Conventionally, upbeat tempos provide for more positive connotations to a song boosting its valence and happiness brought to listeners. However, our results only found that idea to be slightly true. 


### If you did a machine learning model, why did you choose this machine learning technique? Does your data have any sensitive/protected attributes that could affect your machine learning model?

Initially, we were thinking about regression analysis as our ML model. We wanted to see which attributes of the songs matter the most in determining the number of weeks a song stays on the Billboard Hot 100. We discovered lasso regression, which we believed was better than linear regression since it avoids overfitting. However, we later realized that maybe our dataset does not fit linear regression, since the output wasn’t really desirable, and we explored FFNN. FFNN is a deep learning model and ideally, it should be able to fit into any dataset given the right model architecture. This addresses the issue with lasso regression.  

Our data come from weekly Billboard Hot 100 rankings and Spotify API song features. Hence, there are no sensitive or protected attributes.
